{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Documentation that simplySTPP Benchmarking <p>       A comprehensive framework for spatial-temporal point process benchmarking,        providing powerful tools and standardized metrics for evaluating predictive models.     </p> Get Started Try a spidy experiment      The Most Advanced Framework for Spatial-Temporal Point Process Analysis    <p>     Built for researchers and practitioners who demand precision, scalability,      and reproducibility in their benchmarking workflows.   </p>"},{"location":"setup/","title":"Getting started","text":""},{"location":"setup/#introduction","title":"Introduction","text":"<p>A flexible benchmarking toolkit for streaming Spatio-Temporal Point-Process models. BenchSTPP is a modular, research-grade framework for end-to-end development, training, and evaluation of Spatio-Temporal Point-Process (STPP) models. It couples declarative YAML configuration with PyTorch Lightning execution, Ray Tune hyper-parameter optimisation, and version-controlled logging to deliver rapid prototyping and rigorous, reproducible benchmarking on streaming event data.</p> <p>For more information visit Our BenchSTPP repository on Github.</p>"},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#with-git","title":"with git","text":"<p>BenchSTPP can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version:</p> 1. Clone the repository locally <p>Start by cloning the repository and navigating into its directory:</p> <pre><code>$ git clone git@github.com:YahyaAalaila/STPPGC.git\n$ cd STPPGC\n</code></pre> 2. Create a virtual environment (optional but recommended) <p>It is highly recommended to use a virtual environment to prevent potential dependency conflicts with other Python projects.</p> <pre><code># Create the Environment (named .venv)\n$ python3 -m venv .venv\n\n# Activate the Environment\n# On WSL, Linux, &amp; macOS\n$ source .venv/bin/activate\n</code></pre> <p>For Windows users</p> <p>It is recommended to use WSL 2 (Windows Subsystem for Linux) for a smoother experience, as some dependencies and commands may not work properly on native Windows.</p> 3. Install dependencies <pre><code>$ pip install -e .\n# To enable the optional Neural-STPP models\n$ pip install -e .[neural]\n</code></pre> 4. verify Installation <p>To verify that BenchSTPP is installed correctly, run a simple test with full YAML file using the following command:</p> <pre><code>$ pip install -e .\n# To enable the optional Neural-STPP models\n$ pip install -e .[neural]\n</code></pre> <p>This should start a training process using the Neuralstpp model without any errors.</p>"},{"location":"setup/#with-pip","title":"with pip","text":"<p>Info</p> <p>this part is under construction</p> <p>Was this content helpful?</p> yes   /  no"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/","title":"Run First Experiment: Tiny Dataset Example","text":""},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#introduction","title":"Introduction","text":"<p>In this section, we will demonstrate how to launch the first deterministic experiment using the NeuralSTPP model on a tiny toy dataset, specifically the Pinwheel dataset. This experiment is designed as a minimal example to validate that the full training pipeline works correctly: data loading, model initialization, logging, and checkpoint creation.</p> <p>The configuration can be defined:</p> <ul> <li> <p>either using separate YAML configuration files,</p> </li> <li> <p>or directly from the command line (CLI) by overriding parameters interactively.</p> </li> </ul> <p>We\u2019ll start with example YAML configurations, then show how to run the experiment, and finally describe all the artifacts that are generated.</p>"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#configuration","title":"Configuration","text":"<p>The STPPGC framework uses Hydra configuration management, which organizes configuration files in directories (e.g., conf/model/, conf/data/, conf/trainer/, etc). Each YAML file defines a logical part of the experiment (model, dataset, or trainer).</p>"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#1-with-yaml-files","title":"1. With YAML files","text":"<p>Below are minimal examples of configuration files for this first experiment:</p> <p>YAML Configuration Files</p> conf/dat/pinwheel.yamlconf/model/neuralstpp.yamlconf/trainer/default.yaml <p><pre><code>    dataset_id: PinwheelHawkes\n    batch_size: 64 # 128\n    max_events: 1024\n\n    train_bsz:   16\n    val_bsz:     64\n    test_bsz:    64\n    ddim: 2\n    num_workers: 13\n    shuffle:     true\n    drop_last:   true\n    pin_memory:  true\n    seed:        42\n    log_normalisation: 1\n</code></pre> This defines a small synthetic dataset of 300 total samples, sufficient for a lightweight deterministic run.</p> <pre><code>    defaults:\n    - /data: pinwheel\n    - _self_\n\n    model_sub_id: jump-cnf\n\n    search_space:\n    hdims:             [64, 64, 64]\n    tpp_hidden_dims:   [8, 20]\n    actfn:             \"softplus\"\n    tpp_cond:          false\n    tpp_style:         \"split\"\n    tpp_actfn:         \"softplus\"\n    share_hidden:      false\n    solve_reverse:     false\n    l2_attn:           false\n    tol:               0.001\n    otreg_strength:    0.0\n    tpp_otreg_strength: 0.0\n    layer_type:        \"concat\"\n    naive_hutch:       false\n    lowvar_trace:      false\n\n    lr:                [1e-3, 1e-5]\n    warmup_itrs:       2\n    num_iterations:    10\n    momentum:          0.9\n    weight_decay:      0.0\n    gradclip:          1.0\n    max_events:        100\n</code></pre> <pre><code>    gpus:         1\n    accelerator: \"cpu\" # If you utilize MacOS\"mps\"\n    max_epochs:   1\n    precision:    32\n    seed:         123\n    ckpt_dir:     \"./checkpoints/PinwheelHawkesSweep\"\n    save_top_k:   1\n    monitor:      \"val_loss\"\n    resume_from:  null\n\n    extra_callbacks:\n    - class_path: callbacks.common.test_scheduler.TestSchedulerCallback\n        init_args:\n        test_every_n_epochs: 5\n</code></pre> <p>Together, these files fully define the experiment. Once saved in your conf/ directory, Hydra automatically merges them when you run the experiment.</p>"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#2-via-command-line-interface-cli-overrides","title":"2. Via Command Line Interface (CLI) Overrides","text":"<p>Alternatively, you can specify all configuration parameters directly from the command line using Hydra's override syntax. This approach is useful for quick tests or when you want to avoid creating multiple YAML files. BUT, How can we write the correct command?</p> <p>The STPPGC framework uses Hydra, which allows hierarchical configuration defined in multiple .yaml files (under configs/ directory). Each file corresponds to a logical module of the experiment:</p> <p><pre><code>    conf/\n    \u2502   config.yaml          # main entry (imports all defaults)\n    \u251c\u2500\u2500\u2500data\n    \u2502       pinwheel.yaml    # dataset description\n    \u2502\n    \u251c\u2500\u2500\u2500hpo\n    \u2502       default.yaml\n    \u2502\n    \u251c\u2500\u2500\u2500logging\n    \u2502       default.yaml\n    \u2502\n    \u251c\u2500\u2500\u2500model\n    \u2502       neuralstpp.yaml  # model architecture, search space, regularization\n    \u2502       ...              # other model configs\n    \u2502\n    \u2514\u2500\u2500\u2500trainer\n            default.yaml     # training strategy (epochs, GPU, seed, etc.)\n</code></pre> So when you launch:</p> <p><pre><code>$ python3 scripts/cli.py --config-name config.yaml ...\n</code></pre> Hydra loads config.yaml as the base configuration, which contains something like:</p> <p><pre><code>defaults:\n    - data: pinwheel\n    - model: neuralstpp\n    - trainer: default\n    - logging: default\n    - hpo: default\n...\n</code></pre> Then, every argument you pass in the CLI in the form of key=value acts as an override of a field in the hierarchical YAML tree.</p> <p>Breakdown of an Override Command</p> <p>Here is an example command that runs the same experiment as defined in the YAML files above:</p> <p><pre><code>$ python3 scripts/cli.py --config-name config.yaml \\\n    trainer.accelerator=cpu trainer.gpus=0 trainer.max_epochs=1 \\\n    hpo.resources.cpu=64 \\\n    model=neuralstpp \\\n    model.search_space.tpp_hidden_dims=[32,32] \\\n    model.search_space.actfn=\"swish\" \\\n    model.search_space.tpp_cond=true \\\n    model.search_space.tpp_style=\"gru\" \\\n    model.search_space.tpp_actfn=\"swish\" \\\n    model.search_space.tol=1e-4 \\\n    model.search_space.lr=1e-3 \\\n    model.search_space.weight_decay=1e-6 \\\n    model.search_space.otreg_strength=1e-4 \\\n    model.search_space.tpp_otreg_strength=1e-4 \\\n    trainer.seed=42\n</code></pre> \ud83d\udca1 How to read this command</p> <p>The logic follows the directory structure:</p> <ul> <li> <p><code>model.search_space.*</code> \u2192 keys under configs/model/neuralstpp.yaml</p> </li> <li> <p><code>data.*</code> \u2192 keys under configs/data/pinwheel.yaml</p> </li> <li> <p><code>trainer.*</code> \u2192 keys under configs/trainer/trainer.yaml</p> </li> </ul> <p>Thus, each <code>.</code> in the CLI corresponds to a hierarchy inside the YAML file. This structure ensures your overrides remain consistent with the developer\u2019s configuration schema.</p>"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#understanding-dot-notation-mapping","title":"Understanding Dot Notation Mapping","text":"<ul> <li>The key to modifying any experiment parameter is understanding how the command line overrides map directly to the configuration files via dot notation. Our configuration is organized hierarchically, typically spanning modules like <code>model.*</code>, <code>data.*</code>, and <code>trainer.*</code>. For example, a configuration file like <code>configs/trainer/trainer.yaml</code> holds settings for the training loop. When you pass an argument like <code>trainer.accelerator=cpu</code> in the CLI, you are explicitly navigating to the <code>accelerator</code> field inside that <code>trainer.yaml</code> file and setting its value. For more complex settings, like architecture parameters, the dots represent nested dictionary levels. For instance, <code>model.search_space.tpp_hidden_dims=[32,32]</code> tells Hydra to look for the <code>tpp_hidden_dims</code> field, which is nested under <code>search_space</code>, within the currently selected model configuration (e.g., <code>neuralstpp.yaml</code>). This consistent mapping ensures that any setting, no matter how deep, can be overridden directly from the CLI. Since CLI overrides always take the highest priority over the base YAML values, this approach ensures the experiment is fully defined and reproducible.</li> </ul>"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#running-experiment","title":"Running Experiment","text":"<p>After preparing the configuration files, the experiment can be launched using a single command in the CLI.</p> Run with YAML Configuration FilesOverriding Configuration Directly from the CLI <p><pre><code>$ python3 scripts/cli.py --config-name config.yaml\n</code></pre> Hydra automatically reads and merges the YAML files model/neuralstpp.yaml, data/pinwheel.yaml, and trainer/trainer.yaml. This command will run a single, deterministic experiment using the Neural STPP model on the Pinwheel dataset.</p> <p><pre><code>python3 scripts/cli.py --config-name config.yaml \\\ndata.dataset_id=Pinwheel \\\ntrainer.accelerator=cpu trainer.gpus=0 trainer.max_epochs=1 \\\nmodel=neuralstpp \\\nmodel.search_space.tpp_hidden_dims=[32,32] \\\nmodel.search_space.actfn=\"swish\" \\\nmodel.search_space.tpp_cond=true \\\nmodel.search_space.tpp_style=\"gru\" \\\nmodel.search_space.tpp_actfn=\"swish\" \\\nmodel.search_space.tol=1e-4 \\\nmodel.search_space.lr=1e-3 \\\nmodel.search_space.weight_decay=1e-6 \\\ntrainer.seed=42\n</code></pre> As we already explained in last part, this command overrides all necessary parameters directly from the CLI, achieving the same effect as the YAML files.</p> Running on Pegasus HPC <p>These experiments were executed on the Pegasus HPC cluster using a custom containerized environment (CUDA, PyTorch, and all dependencies).</p> <p>The container image was built and deployed as described in the document: \ud83d\udcc4 Custom Software Environment for BenchSTPP \u2014 Download the Full PDF Guide</p> <p>For Pegasus users, you can directly load the image and run the same experiment with: <pre><code>srun --partition=A100-80GB \\\n --gres=gpu:1 \\\n --cpus-per-task=128 \\\n --mem=48G \\\n --time=12:00:00 \\\n --container-image=/netscratch/drief/images/lightning-stpp.sqsh \\\n --container-workdir=\"$(pwd)\" \\\n --container-mounts=/netscratch/$USER/projects/STPPGC:/netscratch/$USER/projects/STPPGC,/ds:/ds:ro,\"$(pwd)\":\"$(pwd)\" \\\n python3 scripts/cli.py --config-name config.yaml trainer.accelerator=gpu trainer.gpus=1 model=neuralstpp data=pinwheel trainer.seed=42\n</code></pre> This ensures a consistent environment across CPU and GPU runs.</p>"},{"location":"Getting_started/Run_First_Experiment/run_first_experiment/#artifacts-overview","title":"Artifacts Overview","text":"<p>After the experiment finishes, several artifacts are automatically generated within the Ray Tune trial directory <code>(e.g., ray_results/Pinwheel_neuralstpp/.../)</code>. These artifacts guarantee reproducibility and traceability.</p> Artifact Type Description Example Path Logs Training and validation statistics, time per iteration, etc., for tracking progress. <code>ray_results/.../events.out.tfevents.*</code> Checkpoints Saved model weights, allowing for future fine-tuning or evaluation without rerunning the entire pipeline. <code>ray_results/.../checkpoint_000000</code> Config Copy The merged configuration (including all CLI overrides) used for this specific run. <code>ray_results/.../config.yaml</code> Metrics Final results, including the validation loss, stored as a JSON file. <code>ray_results/.../metrics.json</code> System Report Hostname, PID, total runtime, and resource usage. Printed in terminal output and logged in the trial directory. Terminal Output Snippet <pre><code>    Trial status: 1 TERMINATED\n    Current time: 2025-09-17 14:17:26. Total running time: 4min 28s\n    Logical resource usage: 64.0/384 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:H100)\n    \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 Trial name                       status                     iter     total time (s)        val_loss \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502 _run_single_trial_a3025_00000    TERMINATED                  1        264.314               404.105 \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n    \ud83d\udd75\ufe0f\u200d\u2642\ufe0f best_trial.last_result keys:\n    dict_keys(['val_loss', 'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'training_iteration', 'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore', 'experiment_tag'])\n    \ud83d\udd75\ufe0f\u200d\u2642\ufe0f full last_result dict:\n    {'checkpoint_dir_name': 'checkpoint_000000',\n    'config': {},\n    'date': '2025-09-17_14-17-26',\n    'done': True,\n    'experiment_tag': '0',\n    'hostname': 'serv-7101',\n    'iterations_since_restore': 1,\n    'node_ip': '192.168.71.181',\n    'pid': 3463648,\n    'should_checkpoint': True,\n    'time_since_restore': 264.3144028186798,\n    'time_this_iter_s': 264.3144028186798,\n    'time_total_s': 264.3144028186798,\n    'timestamp': 1758111446,\n    'training_iteration': 1,\n    'trial_id': 'a3025_00000',\n    'val_loss': 404.104736328125}\n</code></pre> <p>The system report (logged as best_trial.last_result) captures all environmental and performance metrics for the trial:</p> <ul> <li> <p>Final Loss: <code>val_loss</code>: 404.1047</p> </li> <li> <p>Total Time: <code>time_total_s</code>: 264.314 seconds (approx. 4min 24s)</p> </li> <li> <p>Host Environment: <code>hostname</code>: serv-7101, pid: 3463648</p> </li> <li> <p>Trial Identity: <code>trial_id</code>: a3025_00000, training_iteration: 1</p> </li> </ul> <p>This level of detail ensures full experiment transparency.</p>"}]}